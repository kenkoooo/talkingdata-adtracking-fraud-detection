{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# おためしWordBatch\n",
    "---\n",
    "メルカリコンペの時はpipでいけなかったような記憶が\n",
    "\n",
    "**結論**  \n",
    "sklearn.feature_extraction.text.HashingVectorizerとかのwrapper．  \n",
    "メルカリコンペのデータでちょっと試した感じだと，sklearnと違ってマルチコア全部使ってくれるっぽい．  \n",
    "メルカリコンペで流行ってたのは計算時間の問題だと思う．多分．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install\n",
    "pip install wordbatchするだけ  \n",
    "要Cython  \n",
    "https://github.com/anttttti/Wordbatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalize text\n",
      "Extract wordbags\n",
      "Normalize text\n",
      "Extract wordbags\n",
      "[0.50758516]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "#from sklearn.linear_model import *\n",
    "#vct= HashingVectorizer()\n",
    "#clf= SGDRegressor()\n",
    "\n",
    "import wordbatch\n",
    "from wordbatch.models import FTRL\n",
    "from wordbatch.extractors import WordBag, WordHash, WordSeq, WordVec\n",
    "wb= wordbatch.WordBatch(extractor=(WordBag, {\"hash_ngrams\":2, \"hash_ngrams_weights\":[0.5, -1.0], \"hash_size\":2**23, \"norm\":'l2', \"tf\":'log', \"idf\":50.0}))\n",
    "# clf = FTRL(alpha=1.0, beta=1.0, L1=0.00001, L2=1.0, D=2 ** 25, iters=1)\n",
    "clf = FTRL()\n",
    "train_texts = [\"Cut down a tree with a herring? It can't be done.\",\n",
    "              \"Don't say that word.\",\n",
    "              \"How can we not say the word if you don't tell us what it is?\"]\n",
    "train_labels = [1, 0, 1]\n",
    "test_texts = [\"Wait! I said it! I said it! Ooh! I said it again!\"]\n",
    "\n",
    "clf.fit(wb.transform(train_texts), train_labels)\n",
    "preds= clf.predict(wb.transform(test_texts))\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalize text\n",
      "Extract wordbags\n",
      "Normalize text\n",
      "Extract wordbags\n",
      "(3, 8388608) (1, 8388608)\n"
     ]
    }
   ],
   "source": [
    "a = wb.transform(train_texts).toarray()\n",
    "b = wb.transform(test_texts).toarray()\n",
    "print(a.shape, b.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4種類のFeature Extracter\n",
    "\n",
    "- **WordHash**  \n",
    "is simply the Scikit-learn HashingVectorizer wrapped with the Wordbatch parallelization, providing multiplied processing speeds\n",
    "- **WordBag**  \n",
    "is a flexible alternative to Wordhash, providing cababilities missing from Scikit-learn, such as IDF and per n-gram order weighting of hashed features, windowed and distance-weighted polynomial interactions, and more transforms for counts.  \n",
    "使うならこれ？WordHashだとn_gramを適用できないのが辛い．  \n",
    "- **WordSeq**  \n",
    "provides sequences of word integers, as used by the deep learning toolkits for input into LSTM models.  \n",
    "kerasのtokenizerとどう違う？  \n",
    "- **WordVec**  \n",
    "provides embedding transforms from words into wordvectors  \n",
    "予め単語ベクトルを与える必要があるっぽい．それを適用するだけ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalize text\n",
      "Extract wordbags\n",
      "  (0, 784967)\t0.9999999866502418\n",
      "  (1, 6931205)\t-0.9999999866502418\n",
      "  (2, 6188862)\t0.9999999866502418\n",
      "Normalize text\n",
      "Extract wordbags\n",
      "  (0, 784967)\t0.41252769709377807\n",
      "  (0, 1216271)\t0.8121833286211833\n",
      "  (0, 6931205)\t-0.41252769709377807\n",
      "  (1, 2052107)\t-0.8121833286211833\n",
      "  (1, 6188862)\t0.41252769709377807\n",
      "  (1, 6931205)\t-0.41252769709377807\n"
     ]
    }
   ],
   "source": [
    "txt1 = [\"Hello\", \"world\", \"Goodbye\"]\n",
    "txt2 = [\"Hello world\", \"Goodbye world\"]\n",
    "params_wb = {\"hash_ngrams\":2, \n",
    "             \"hash_ngrams_weights\":[0.5, -1.0], \n",
    "             \"hash_size\":2**23, \n",
    "             \"norm\":'l2', \n",
    "             \"tf\":'log', \n",
    "             \"idf\":50.0}\n",
    "wb = wordbatch.WordBatch(extractor=(WordBag, params_wb))\n",
    "print(wb.transform(txt1))\n",
    "print(wb.transform(txt2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalize text\n",
      "Extract wordhashes\n",
      "  (0, 784967)\t1.0\n",
      "  (1, 639749)\t-1.0\n",
      "  (2, 945982)\t1.0\n",
      "Normalize text\n",
      "Extract wordhashes\n",
      "  (0, 639749)\t-0.7071067811865475\n",
      "  (0, 784967)\t0.7071067811865475\n",
      "  (1, 639749)\t-0.7071067811865475\n",
      "  (1, 945982)\t0.7071067811865475\n"
     ]
    }
   ],
   "source": [
    "txt1 = [\"Hello\", \"world\", \"Goodbye\"]\n",
    "txt2 = [\"Hello world\", \"Goodbye world\"]\n",
    "params_wb = {\"norm\":'l2'}\n",
    "wb = wordbatch.WordBatch(extractor=(WordHash, params_wb))\n",
    "print(wb.transform(txt1))\n",
    "print(wb.transform(txt2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalize text\n",
      "Extract wordseqs\n",
      "[[1], [2], [3], [1]]\n",
      "Normalize text\n",
      "Extract wordseqs\n",
      "[[1, 2], [3, 2]]\n"
     ]
    }
   ],
   "source": [
    "txt1 = [\"Hello\", \"world\", \"Goodbye\", \"Hello\"]\n",
    "txt2 = [\"Hello world\", \"Goodbye world\"]\n",
    "params_wb = {\"norm\":'l1'}\n",
    "wb = wordbatch.WordBatch(extractor=(WordSeq, params_wb))\n",
    "print(wb.transform(txt1))\n",
    "print(wb.transform(txt2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 784967)\t1.0\n",
      "  (1, 639749)\t-1.0\n",
      "  (2, 945982)\t1.0\n",
      "  (3, 784967)\t1.0\n",
      "  (0, 639749)\t-0.7071067811865475\n",
      "  (0, 784967)\t0.7071067811865475\n",
      "  (1, 639749)\t-0.7071067811865475\n",
      "  (1, 945982)\t0.7071067811865475\n"
     ]
    }
   ],
   "source": [
    "txt1 = [\"Hello\", \"world\", \"Goodbye\", \"Hello\"]\n",
    "txt2 = [\"Hello world\", \"Goodbye world\"]\n",
    "params_hv = {\"norm\":'l1'}\n",
    "\n",
    "vct = HashingVectorizer(params_hv)\n",
    "print(vct.fit_transform(txt1))\n",
    "print(vct.fit_transform(txt2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "py36con",
   "language": "python",
   "name": "py36con"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
